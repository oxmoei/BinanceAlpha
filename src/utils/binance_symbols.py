import requests
import os
import json
from datetime import datetime
import re
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

def fetch_symbols():
    """ä»Binanceè·å–æ‰€æœ‰äº¤æ˜“å¯¹"""
    response = requests.get('https://api.binance.com/api/v3/exchangeInfo')
    data = response.json()
    symbols = [s['symbol'] for s in data['symbols']]
    return symbols

def extract_token_names(symbols):
    """ä»äº¤æ˜“å¯¹ä¸­æå–é€šè¯(token)åç§°"""
    # å®šä¹‰å¸¸è§çš„è®¡ä»·è´§å¸
    quote_currencies = ['BTC', 'ETH', 'USDT', 'BUSD', 'BNB', 'USDC', 'EUR', 'TRY', 'FDUSD', 'TUSD', 'JPY', 'ARS', 'MXN', 'BRL', 'AEUR', 'PLN', 'RUB', 'RON', 'VAI', 'EURI', 'CZK', 'COP']
    
    # æ·»åŠ ä¸€äº›å·²çŸ¥çš„ç‰¹æ®Šæƒ…å†µ
    special_cases = {
        'BTCDOMUSDT': 'BTCDOM',  # æ¯”ç‰¹å¸ä¸»å¯¼åœ°ä½æŒ‡æ•°
        'BTCDOMBUSD': 'BTCDOM',
        'DEFIUSDT': 'DEFI',      # DeFiæŒ‡æ•°
        'DEFIBUSD': 'DEFI',
        # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ·»åŠ æ›´å¤šç‰¹æ®Šæƒ…å†µ
    }
    
    tokens = set()
    unmatched_symbols = []
    
    # é¦–å…ˆå¤„ç†ç‰¹æ®Šæƒ…å†µ
    for symbol in symbols:
        if symbol in special_cases:
            tokens.add(special_cases[symbol])
            continue
    
        # å°è¯•ä½¿ç”¨å¸¸è§„æ–¹æ³•æå–tokenåç§°
        matched = False
        for quote in quote_currencies:
            if symbol.endswith(quote):
                token = symbol[:-len(quote)]
                if token and re.match(r'^[A-Z0-9]+$', token):  # ç¡®ä¿tokenåªåŒ…å«å¤§å†™å­—æ¯å’Œæ•°å­—
                    tokens.add(token)
                    matched = True
                    break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè®°å½•ä¸‹æ¥ä¾›åç»­å¤„ç†
        if not matched:
            unmatched_symbols.append(symbol)
    
    # å¤„ç†æœªåŒ¹é…çš„äº¤æ˜“å¯¹
    if unmatched_symbols:
        logger.info(f"å‘ç°{len(unmatched_symbols)}ä¸ªæœªåŒ¹é…çš„äº¤æ˜“å¯¹ï¼š{', '.join(unmatched_symbols[:10])}" + 
                   (f"...ç­‰å…±{len(unmatched_symbols)}ä¸ª" if len(unmatched_symbols) > 10 else ""))
        
        # å°è¯•æ›´å¤æ‚çš„æå–æ–¹æ³•
        for symbol in unmatched_symbols:
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆåŸºæœ¬æ ¼å¼è¦æ±‚
            if re.match(r'^[A-Z0-9]+$', symbol):
                # å°è¯•å°†symbolæœ¬èº«ä½œä¸ºtokenæ·»åŠ ï¼ˆå¦‚æœå®ƒä¸æ˜¯è®¡ä»·è´§å¸ï¼‰
                if symbol not in quote_currencies:
                    tokens.add(symbol)
                
                # å¤„ç†ç±»ä¼¼1000TOKENå½¢å¼çš„token
                number_token_match = re.match(r'^(\d+)([A-Z]+)$', symbol)
                if number_token_match:
                    token_name = number_token_match.group(2)
                    if token_name not in quote_currencies:
                        tokens.add(token_name)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šå¤æ‚çš„æå–è§„åˆ™
            # ä¾‹å¦‚é’ˆå¯¹TOKEN1TOKEN2è¿™ç§æƒ…å†µçš„å¤„ç†
    
    return sorted(list(tokens))

def get_existing_tokens():
    """è·å–å·²å­˜åœ¨çš„tokenåˆ—è¡¨"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    symbols_dir = os.path.join(root_dir, 'symbols')
    
    # åˆ›å»ºsymbolsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(symbols_dir):
        os.makedirs(symbols_dir)
        return []
    
    # è·å–å·²æœ‰çš„symbolsæ–‡ä»¶åˆ—è¡¨
    symbol_files = [f for f in os.listdir(symbols_dir) if os.path.isfile(os.path.join(symbols_dir, f)) and f.endswith('.json')]
    if not symbol_files:
        return []
    
    # è¯»å–æœ€æ–°çš„symbolsæ–‡ä»¶
    latest_file = sorted(symbol_files)[-1]
    with open(os.path.join(symbols_dir, latest_file), 'r') as f:
        existing_tokens = json.load(f)
    
    return existing_tokens

def get_raw_symbols_file():
    """è·å–æœ€æ–°çš„åŸå§‹symbolsæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    symbols_dir = os.path.join(root_dir, 'symbols', 'raw')
    
    # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œè¿”å›None
    if not os.path.exists(symbols_dir):
        return None
    
    # è·å–æ‰€æœ‰çš„raw symbolsæ–‡ä»¶
    symbol_files = [f for f in os.listdir(symbols_dir) if os.path.isfile(os.path.join(symbols_dir, f)) and f.endswith('.json')]
    if not symbol_files:
        return None
    
    # è¿”å›æœ€æ–°çš„æ–‡ä»¶è·¯å¾„
    latest_file = sorted(symbol_files)[-1]
    return os.path.join(symbols_dir, latest_file)

def get_cex_tokens():
    """è·å–å¸å®‰CEXä¸Šå·²ä¸Šçº¿çš„token"""
    try:
        # è·å–æ‰€æœ‰äº¤æ˜“å¯¹
        symbols = fetch_symbols()
        
        # æå–tokenåç§°
        cex_tokens = extract_token_names(symbols)
        
        logger.info(f"ä»å¸å®‰è·å–åˆ°{len(cex_tokens)}ä¸ªä¸Šçº¿token")
        return cex_tokens
    except Exception as e:
        logger.error(f"è·å–å¸å®‰CEXä¸Šçº¿tokenæ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []

def check_token_listing_status(token, listed_tokens=None):
    """æ£€æŸ¥tokenæ˜¯å¦å·²ç»åœ¨å¸å®‰ç°è´§ä¸Šçº¿
    
    Args:
        token (str): è¦æ£€æŸ¥çš„tokenç¬¦å·
        listed_tokens (dict, optional): å·²è·å–çš„ä¸Šçº¿tokenä¿¡æ¯å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™é‡æ–°è·å–
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹ä¿¡æ¯çš„å­—å…¸ï¼š
            - is_listed (bool): æ˜¯å¦å·²ä¸Šçº¿
            - listing_type (str, optional): ä¸Šçº¿ç±»å‹ï¼Œ'standard'æˆ–'1000x'æˆ–None
            - listed_as (str, optional): å®é™…ä¸Šçº¿çš„ç¬¦å·åç§°
    """
    # ç¡®ä¿tokenæ˜¯å¤§å†™
    token = token.upper() if token else ""
    
    if not token:
        return {"is_listed": False}
    
    # å¦‚æœæ²¡æœ‰æä¾›listed_tokensï¼Œåˆ™è·å–æœ€æ–°æ•°æ®
    if not listed_tokens:
        try:
            listed_tokens_data = update_tokens()
            if not listed_tokens_data:
                logger.warning("è·å–ä¸Šçº¿tokenåˆ—è¡¨å¤±è´¥")
                return {"is_listed": False}
        except Exception as e:
            logger.error(f"æ£€æŸ¥tokenä¸Šçº¿çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
            return {"is_listed": False}
    else:
        listed_tokens_data = listed_tokens
    
    # è·å–æ ‡å‡†å½¢å¼çš„tokené›†åˆï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    standard_tokens_set = {t.upper() for t in listed_tokens_data.get('standard_tokens', [])}
    if not standard_tokens_set and 'all_tokens' in listed_tokens_data:
        # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„standard_tokensï¼Œä½¿ç”¨all_tokens
        standard_tokens_set = {t.upper() for t in listed_tokens_data.get('all_tokens', [])}
    
    # è·å–1000Tokenå½¢å¼çš„æ˜ å°„
    thousand_tokens = listed_tokens_data.get('thousand_tokens', [])
    thousand_tokens_map = {real_token.upper(): full_name.upper() 
                         for full_name, real_token in thousand_tokens}
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡å‡†å½¢å¼token
    if token in standard_tokens_set:
        return {
            "is_listed": True,
            "listing_type": "standard",
            "listed_as": token
        }
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯1000Tokenå¯¹åº”çš„ä»£å¸
    elif token in thousand_tokens_map:
        return {
            "is_listed": True,
            "listing_type": "1000x",
            "listed_as": thousand_tokens_map[token]
        }
    
    return {"is_listed": False}

def prepare_token_listing_data(tokens_data):
    """é¢„å¤„ç†å¸å®‰ä¸Šçº¿tokenæ•°æ®ï¼Œåˆ†ç¦»æ ‡å‡†tokenå’Œ1000xå½¢å¼token
    
    Args:
        tokens_data (dict): update_tokens()è¿”å›çš„tokenæ•°æ®
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - standard_tokens (list): æ ‡å‡†å½¢å¼çš„tokenåˆ—è¡¨
            - thousand_tokens (list): 1000xå½¢å¼çš„tokenå…ƒç»„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç»„ä¸º(full_name, real_token)
            - cex_info_message (str): æ ¼å¼åŒ–çš„CEXä¸Šçº¿ä¿¡æ¯
    """
    if not tokens_data or not isinstance(tokens_data, dict):
        logger.warning("æ— æ•ˆçš„tokenæ•°æ®")
        return {
            "standard_tokens": [],
            "thousand_tokens": [],
            "cex_info_message": "æ— æœ‰æ•ˆæ•°æ®"
        }
    
    # è·å–CEXä¸Šçº¿çš„tokenåˆ—è¡¨
    cex_tokens = tokens_data.get('cex_tokens', [])
    if cex_tokens is None:
        cex_tokens = []
        logger.warning("cex_tokensä¸ºNoneï¼Œä½¿ç”¨ç©ºåˆ—è¡¨ä»£æ›¿")
    
    # é¢„å¤„ç†1000Tokenå½¢å¼çš„ä»£å¸åç§°
    thousand_tokens = []
    standard_tokens = []
    
    for token in cex_tokens:
        if token.startswith('1000') and len(token) > 4:
            # æå–1000åé¢çš„å®é™…ä»£å¸åç§°
            real_token = token[4:]
            thousand_tokens.append((token, real_token))  # ä¿å­˜å…ƒç»„(å®Œæ•´åç§°, å®é™…ä»£å¸åç§°)
        else:
            standard_tokens.append(token)
    
    # æ„å»ºä¸Šçº¿ä¿¡æ¯æ¶ˆæ¯
    cex_info = "ğŸ”” å¸å®‰ç°è´§å·²ä¸Šçº¿Tokenåˆ—è¡¨ï¼š\n\n"
    
    # æ·»åŠ å¸¸è§„token
    if standard_tokens:
        cex_info += "ğŸ“Š æ ‡å‡†Tokenï¼š\n"
        for i, token in enumerate(sorted(standard_tokens)[:20], 1):
            cex_info += f"{i}. {token}\n"
        if len(standard_tokens) > 20:
            cex_info += f"...ä»¥åŠå…¶ä»– {len(standard_tokens)-20} ä¸ªtoken\n"
        cex_info += "\n"
    
    # æ·»åŠ 1000Tokenå½¢å¼çš„ä¿¡æ¯
    if thousand_tokens:
        cex_info += "ğŸ’° 1000Tokenå½¢å¼ï¼š\n"
        for i, (full_name, token_name) in enumerate(sorted(thousand_tokens), 1):
            cex_info += f"{i}. {full_name} (åŸå§‹: {token_name})\n"
        cex_info += "\n"
    
    return {
        "standard_tokens": standard_tokens,
        "thousand_tokens": thousand_tokens,
        "cex_info_message": cex_info
    }

def update_tokens():
    """æ›´æ–°tokenåˆ—è¡¨å¹¶è¿”å›æ–°tokenï¼Œåªæœ‰åœ¨äº¤æ˜“å¯¹åˆ—è¡¨å˜åŒ–æ—¶æ‰ä¿å­˜"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    symbols_dir = os.path.join(root_dir, 'symbols')
    raw_symbols_dir = os.path.join(symbols_dir, 'raw')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(symbols_dir):
        os.makedirs(symbols_dir)
    if not os.path.exists(raw_symbols_dir):
        os.makedirs(raw_symbols_dir)
    
    # è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´
    current_datetime = datetime.now().strftime('%Y%m%d-%H%M%S')
    
    # è·å–ç°æœ‰çš„tokens
    existing_tokens = get_existing_tokens()
    
    # è·å–æ‰€æœ‰äº¤æ˜“å¯¹
    all_symbols = fetch_symbols()
    
    # æ£€æŸ¥äº¤æ˜“å¯¹åˆ—è¡¨æ˜¯å¦æœ‰å˜åŒ–
    symbols_changed = True
    latest_raw_file = get_raw_symbols_file()
    
    if latest_raw_file:
        try:
            with open(latest_raw_file, 'r') as f:
                previous_symbols = json.load(f)
            
            # æ¯”è¾ƒæ–°æ—§äº¤æ˜“å¯¹åˆ—è¡¨
            if set(all_symbols) == set(previous_symbols):
                symbols_changed = False
                logger.info("äº¤æ˜“å¯¹åˆ—è¡¨æœªå‘ç”Ÿå˜åŒ–ï¼Œä½¿ç”¨å·²æœ‰tokenåˆ—è¡¨")
        except Exception as e:
            logger.warning(f"è¯»å–ä¸Šä¸€æ¬¡çš„äº¤æ˜“å¯¹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}ï¼Œå°†é‡æ–°ä¿å­˜")
    
    # å¦‚æœäº¤æ˜“å¯¹åˆ—è¡¨æœ‰å˜åŒ–ï¼Œä¿å­˜åŸå§‹æ•°æ®å’Œæå–çš„token
    if symbols_changed:
        # ä¿å­˜åŸå§‹äº¤æ˜“å¯¹åˆ—è¡¨
        raw_filename = f"raw-symbols-{current_datetime}.json"
        raw_filepath = os.path.join(raw_symbols_dir, raw_filename)
        with open(raw_filepath, 'w') as f:
            json.dump(all_symbols, f, indent=2)
        
        # æå–tokenåç§°
        token_names = extract_token_names(all_symbols)
        
        # ä¿å­˜æå–çš„tokenåˆ—è¡¨
        filename = f"symbol.json"
        filepath = os.path.join(symbols_dir, filename)
        with open(filepath, 'w') as f:
            json.dump(token_names, f, indent=2)
        
        # æ‰¾å‡ºæ–°å¢çš„tokenï¼ˆä¸åœ¨å·²å­˜åœ¨åˆ—è¡¨ä¸­çš„ï¼‰
        new_tokens = [t for t in token_names if t not in existing_tokens]
        
        # è·å–CEXä¸Šçº¿çš„token
        cex_tokens = get_cex_tokens()
        
        # é¢„å¤„ç†tokenæ•°æ®
        token_data = prepare_token_listing_data({"cex_tokens": cex_tokens})
        
        return {
            "all_tokens": token_names,
            "new_tokens": new_tokens,
            "existing_tokens": existing_tokens,
            "cex_tokens": cex_tokens,
            "standard_tokens": token_data["standard_tokens"],
            "thousand_tokens": token_data["thousand_tokens"],
            "cex_info_message": token_data["cex_info_message"],
            "file_path": filepath,
            "symbols_changed": True
        }
    else:
        # å¦‚æœäº¤æ˜“å¯¹åˆ—è¡¨æ²¡æœ‰å˜åŒ–ï¼Œè¿”å›å·²æœ‰çš„tokenåˆ—è¡¨
        # å³ä½¿äº¤æ˜“å¯¹åˆ—è¡¨æ²¡å˜ï¼Œä¹Ÿè¦è·å–æœ€æ–°çš„CEXä¸Šçº¿token
        cex_tokens = get_cex_tokens()
        
        # é¢„å¤„ç†tokenæ•°æ®
        token_data = prepare_token_listing_data({"cex_tokens": cex_tokens})
        
        return {
            "all_tokens": existing_tokens,
            "new_tokens": [],
            "existing_tokens": existing_tokens,
            "cex_tokens": cex_tokens,
            "standard_tokens": token_data["standard_tokens"],
            "thousand_tokens": token_data["thousand_tokens"],
            "cex_info_message": token_data["cex_info_message"],
            "file_path": latest_raw_file,
            "symbols_changed": False
        }

def is_token_listed(symbol: str, symbol_list_path: str = None) -> bool:
    """
    æ£€æŸ¥tokenæ˜¯å¦å·²åœ¨å¸å®‰ä¸Šçº¿ï¼Œé€šè¿‡ç›´æ¥è¯»å–symbol.jsonæ–‡ä»¶
    
    Args:
        symbol: è¦æ£€æŸ¥çš„tokenç¬¦å·
        symbol_list_path: symbol.jsonæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        
    Returns:
        bool: æ˜¯å¦å·²ä¸Šçº¿
    """
    # ç¡®ä¿symbolæ˜¯å¤§å†™
    symbol = symbol.upper() if symbol else ""
    
    if not symbol:
        return False
        
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if not symbol_list_path:
        # è·å–é¡¹ç›®æ ¹ç›®å½•çš„symbolsç›®å½•
        root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        symbol_list_path = os.path.join(root_dir, 'symbols', 'symbol.json')
    
    try:
        # è¯»å–symbol.jsonæ–‡ä»¶
        with open(symbol_list_path, 'r') as f:
            listed_tokens = json.load(f)
            
        # æ£€æŸ¥æ ‡å‡†å½¢å¼token
        if symbol in listed_tokens:
            return True
            
        # æ£€æŸ¥1000xå½¢å¼token
        for token in listed_tokens:
            if token.startswith('1000') and token[4:] == symbol:
                return True
                
        return False
        
    except Exception as e:
        print(f"æ£€æŸ¥tokenä¸Šçº¿çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        return False

if __name__ == "__main__":
    # å½“ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œæ—¶æ‰§è¡Œçš„ä»£ç 
    result = update_tokens()
    
    if result["symbols_changed"]:
        print(f"äº¤æ˜“å¯¹åˆ—è¡¨å·²æ›´æ–°")
        print(f"å·²å­˜åœ¨çš„tokenæ•°é‡: {len(result['existing_tokens'])}")
        print(f"æå–å‡ºçš„tokenæ•°é‡: {len(result['all_tokens'])}")
        print(f"æ‰€æœ‰tokenå·²ä¿å­˜åˆ°: {result['file_path']}")
        print(f"æ–°å¢tokenæ•°é‡: {len(result['new_tokens'])}")
        
        if result['new_tokens']:
            print("æ–°å¢token:")
            print(json.dumps(result['new_tokens'], indent=2))
        else:
            print("æ²¡æœ‰æ–°å¢çš„token")
    else:
        print("äº¤æ˜“å¯¹åˆ—è¡¨æœªå‘ç”Ÿå˜åŒ–ï¼Œæ— éœ€æ›´æ–°tokenåˆ—è¡¨")
        print(f"ç°æœ‰tokenæ•°é‡: {len(result['all_tokens'])}") 